{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c823cc2f",
   "metadata": {},
   "source": [
    "### 1. Linear Model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc96f9e",
   "metadata": {},
   "source": [
    "Consider a vector $y_{n \\times 1}$ (**dependent, endogenous**) that we will try to explain using the information contained in a matrix $X_{n\\times k}$ (**independent, exogenous**). We assume a linear relationship between $X$ and $y$, measured by $k$ effects stored in $b_{k\\times 1}$. There will be an estimation error $e_{n \\times 1}$ (**residuals**). Note that we are not yet using any of the seven assumptions.\n",
    "\n",
    "$$ y = Xb + e  $$\n",
    "\n",
    "The criteria to call this a \"good\" regression is to consider some \"criterion function\" that depends on the residuals. \n",
    "\n",
    "$$S(b) = f(e)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d509311",
   "metadata": {},
   "source": [
    "### 2. OLS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa5e9a6",
   "metadata": {},
   "source": [
    "OLS considers the criterion function to be sum of squares of the residuals. This also represents the square of the norm of the residual vector:\n",
    "\n",
    "$$S(b) = \\sum_n e_i^2 = e^Te = \\|e \\|^2 $$\n",
    "\n",
    "Minimizing:\n",
    "\n",
    "$$S(b) = e^Te = (y - Xb)^T(y - Xb) $$\n",
    "\n",
    "$$\\frac{dS(b)}{db} = 2(y - Xb)^TX = 0 = X^T(y - Xb) $$\n",
    "\n",
    "Obtaning the **normal equations**, this is the set of equations to be solved:\n",
    "\n",
    "$$ X^Ty = X^TXb$$\n",
    "\n",
    "Granted that $X^TX$ has full rank, solving for $b$:\n",
    "\n",
    "$$ b = (X^TX)^{-1}X^Ty  $$\n",
    "\n",
    "As a side note, using the normal equations and replacing $y$ by the linear model\n",
    "\n",
    "$$ X^Ty = X^T(Xb + e)= X^TXb + X^Te = X^TXb \\quad \\text{then, $X^Te =0$}$$\n",
    "\n",
    "If $X$ has a column of ones, meaning that this model has an intercept, it follows that $i^Te = 0$ and $\\bar{e}=0$. This implies that the covariance between the columns of $X$ and $e$ can be evaluated by $X^Te$, which is always 0. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9300dc47",
   "metadata": {},
   "source": [
    "### 3. Geometrical properties ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a7941",
   "metadata": {},
   "source": [
    "Replacing the estimated $b$ in the linear model:\n",
    "\n",
    "$$ y = Xb + e = y + X(X^TX)^{-1}X^Ty + e $$\n",
    "\n",
    "Defining $H = X(X^TX)^{-1}X^T$ we check that this matrix projects a vector on the column space of $X$. What is the dimension of this space?\n",
    "\n",
    "$$rank(H) = Tr(H) = Tr(X(X^TX)^{-1}X^T) = Tr((X^TX)^{-1}X^TX) = Tr(I_k) = k $$\n",
    "\n",
    "The estimated values of $y$ are in the column space of $X$, which is lower dimensional. Then, the residual vector $e$ can be obtained by another projection of $y$ using the matrix $M=I-H$, a $n-k$ dimensional space orthogonal to $X$. This is why it is said that **the residuals of the estimation have n-k degrees of freedom**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad55a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
