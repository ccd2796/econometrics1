{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bebe72e",
   "metadata": {},
   "source": [
    "### 1. Maximum Likelihood ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b99d8c",
   "metadata": {},
   "source": [
    "From statistics, it was stablished that one of many methods to find estimators for the parameters of the distribution of a certain random variable was to maximize the likelihood of obtaining a particular sample. In econometrics, besides estimating parameters about the distribution, we will estimate parameters regarding our model $y = X \\beta + \\epsilon$.\n",
    "\n",
    "We start assuming we know the distribution of $\\epsilon$ with unknown parameters $\\psi$ (could be the mean, variance, kurtosis, rate, scale, etc.). Given an IID sample of size $n$ of this random variable, we can write its likelihood as\n",
    "\n",
    "$$L(\\psi, \\beta) = L(\\psi, y - X \\beta) = L(\\psi, \\beta) = \\prod^n f(\\psi, \\beta) \\quad \\text{, where $f$ is the pdf of $\\epsilon$} $$\n",
    "\n",
    "$$l(\\psi, \\beta) = \\log(L(\\psi, \\beta)) = \\sum^n \\log(f(\\psi, \\beta)) $$\n",
    "\n",
    "$$l(\\psi, \\beta) = l(\\theta)  $$\n",
    "\n",
    "From now on, $\\theta$ will be used to represent the vector of both the parameters from the model and from the distribution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80ac27",
   "metadata": {},
   "source": [
    "### 2. Information matrix ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88624e84",
   "metadata": {},
   "source": [
    "The ML estimators for $\\theta$, obtained by maximizing the likelihood, will follow a distribution that converges to a normal:\n",
    "\n",
    "$$\\sqrt{n}(\\hat{\\theta}_{ML} - \\theta_0) \\overset{d}{\\to} N(0, I_0^{-1}).\n",
    " $$\n",
    "\n",
    "From the book:\n",
    "\n",
    "<blockquote>Here $I_0$ is the asymptotic information matrix evaluated at $ \\theta_0 $ that is,\n",
    "$ I_0 = \\lim_{n \\to \\infty} \\left(\\frac{1}{n} I_n(\\theta_0)\\right) $\n",
    "\n",
    "$$I_n(\\theta_0) = E\\left[ \\frac{\\partial l}{\\partial \\theta} \\frac{\\partial l}{\\partial \\theta'} \\right] = -E\\left[ \\frac{\\partial^2 l}{\\partial \\theta \\partial \\theta'} \\right] $$\n",
    "    \n",
    "is the information matrix (evaluated at $\\theta = \\theta_0$ for sample size $n$ of the data $(y, X)$\n",
    "    \n",
    "</blockquote>\n",
    "\n",
    "The information matrix is the second derivative of the likelihood function with respect to the parameters. This represents the \"curvature\" of the function with respect to $\\theta$. If this is high, then small changes in the parameters will impact significantly the likelihood. This means that the data is highly informative about the estimated parameters. It is intuitive to conclude that the more informative the data is, the less variable the estimated parameter will be.\n",
    "\n",
    "\n",
    "Then, the $\\hat{\\theta}_{ML}$ has an approximate normal distribution\n",
    "\n",
    "$$\\hat{\\theta}_{ML} \\approx N\\left(\\theta_0, I_n^{-1}(\\hat{\\theta}_{ML})\\right)$$\n",
    "\n",
    "**And we can use our usual tests.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fecd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
