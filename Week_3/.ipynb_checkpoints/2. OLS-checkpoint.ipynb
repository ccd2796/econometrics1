{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c823cc2f",
   "metadata": {},
   "source": [
    "### 1. Linear Model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc96f9e",
   "metadata": {},
   "source": [
    "Consider a vector $y_{n \\times 1}$ (**dependent, endogenous**) that we will try to explain using the information contained in a matrix $X_{n\\times k}$ (**independent, exogenous**). We assume a linear relationship between $X$ and $y$, measured by $k$ effects stored in $b_{k\\times 1}$. There will be an estimation error $e_{n \\times 1}$ (**residuals**). Note that we are not yet using any of the seven assumptions.\n",
    "\n",
    "$$ y = Xb + e  $$\n",
    "\n",
    "The criteria to call this a \"good\" regression is to consider some \"criterion function\" that depends on the residuals. \n",
    "\n",
    "$$S(b) = f(e)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d509311",
   "metadata": {},
   "source": [
    "### 2. OLS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa5e9a6",
   "metadata": {},
   "source": [
    "OLS considers the criterion function to be sum of squares of the residuals. This also represents the square of the norm of the residual vector:\n",
    "\n",
    "$$S(b) = \\sum_n e_i^2 = e^Te = \\|e \\|^2 $$\n",
    "\n",
    "Minimizing:\n",
    "\n",
    "$$S(b) = e^Te = (y - Xb)^T(y - Xb) $$\n",
    "\n",
    "$$\\frac{dS(b)}{db} = 2(y - Xb)^TX = 0 = X^T(y - Xb) $$\n",
    "\n",
    "Obtaning the **normal equations**, this is the set of equations to be solved:\n",
    "\n",
    "$$ X^Ty = X^TXb$$\n",
    "\n",
    "Granted that $X^TX$ has full rank, solving for $b$:\n",
    "\n",
    "$$ b = (X^TX)^{-1}X^Ty  $$\n",
    "\n",
    "As a side note, using the normal equations and replacing $y$ by the linear model\n",
    "\n",
    "$$ X^Ty = X^T(Xb + e)= X^TXb + X^Te = X^TXb \\quad \\text{then, $X^Te =0$}$$\n",
    "\n",
    "If $X$ has a column of ones, meaning that this model has an intercept, it follows that $i^Te = 0$ and $\\bar{e}=0$. This implies that the covariance between the columns of $X$ and $e$ can be evaluated by $X^Te$, which is always 0. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9300dc47",
   "metadata": {},
   "source": [
    "### 3. Geometrical properties ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a7941",
   "metadata": {},
   "source": [
    "Replacing the estimated $b$ in the linear model:\n",
    "\n",
    "$$ y = Xb + e = y + X(X^TX)^{-1}X^Ty + e $$\n",
    "\n",
    "Defining $H = X(X^TX)^{-1}X^T$ we check that this matrix projects a vector on the column space of $X$. What is the dimension of this space?\n",
    "\n",
    "$$rank(H) = Tr(H) = Tr(X(X^TX)^{-1}X^T) = Tr((X^TX)^{-1}X^TX) = Tr(I_k) = k $$\n",
    "\n",
    "The estimated values of $y$ are in the column space of $X$, which is lower dimensional. Then, the residual vector $e$ can be obtained by another projection of $y$ using the matrix $M=I-H$, a $n-k$ dimensional space orthogonal to $X$. This is why it is said that **the residuals of the estimation have n-k degrees of freedom**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45386841",
   "metadata": {},
   "source": [
    "### 4. Statistical properties of b ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06231c6",
   "metadata": {},
   "source": [
    "The estimator $b$ is a random variable. What can we say about its properties? (note the use of $\\stackrel{\\text{A*}}{=}$ to highlight the assumption used)\n",
    "\n",
    "$$b = (X^TX)^{-1}X^Ty \\stackrel{\\text{A6}}{=} (X^TX)^{-1}X^T( X\\beta  + \\epsilon) = \\beta + (X^TX)^{-1}X^T\\epsilon $$\n",
    "\n",
    "**Expectation:**\n",
    "\n",
    "$$ E[b] = \\beta + E[(X^TX)^{-1}X^T\\epsilon] \\stackrel{\\text{A1}}{=} \\beta + (X^TX)^{-1}X^TE[\\epsilon]  \\stackrel{\\text{A2}}{=} \\beta   $$\n",
    "\n",
    "Under assumptions, $b$ is unbiased.\n",
    "\n",
    "\n",
    "**Variance:**\n",
    "\n",
    "$$ Var(b) = E[(b-E[b])(b-E[b])^T] = E[(b-\\beta)(b-\\beta)^T] = E[ (X^TX)^{-1}X^T\\epsilon \\epsilon^T X (X^TX)^{-1}]  $$\n",
    "\n",
    "$$E[ (X^TX)^{-1}X^T\\epsilon \\epsilon^T X (X^TX)^{-1}]  \\stackrel{\\text{A1}}{=}  (X^TX)^{-1}X^TE[\\epsilon \\epsilon^T] X (X^TX)^{-1} $$\n",
    "\n",
    "$$ (X^TX)^{-1}X^TE[\\epsilon \\epsilon^T] X (X^TX)^{-1}  \\stackrel{\\text{A3,A4}}{=}  \\sigma^2(X^TX)^{-1}  $$\n",
    "\n",
    "Under assumptions, variance of $b$ is.\n",
    "\n",
    "$$ Var(b) = \\sigma^2(X^TX)^{-1} $$\n",
    "\n",
    "The diagonal elements contain the variance of the estimators $b_j$, and the off diagonal represent the covariance between estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f9b48",
   "metadata": {},
   "source": [
    "### 5. Distribution of e ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300c9ad",
   "metadata": {},
   "source": [
    "Recall that $e$ can be constructed from $y$ by projecting it using the $M$ matrix, with $rank(M)=n-k$ (note the use of $\\stackrel{\\text{A*}}{=}$ to highlight the assumption used)\n",
    "\n",
    "$$ e = My \\stackrel{\\text{A6}}{=} M(X\\beta + \\epsilon) = (I-H) X\\beta + M\\epsilon $$\n",
    "\n",
    "Naturally, projecting $X$ on its own column space will yield $X$\n",
    "\n",
    "$$ e = (X-X)\\beta + M\\epsilon = M\\epsilon $$\n",
    "\n",
    "Under assumption 6, the residual vector is a projection of the disturbances. This space has $n-k$ degrees of freedom.\n",
    "\n",
    "Furthermore, using assumption 2-4, and 7:\n",
    "\n",
    "$$\\epsilon \\sim N_n(0, \\sigma^2I_n) $$\n",
    "\n",
    "$$ e = M\\epsilon \\sim N_n(0, \\sigma^2M) $$\n",
    "\n",
    "The residual vector $e$ follows a degenerate normal distribution. However, we can find a distribution for the squared norm of this vector. Using previous results:\n",
    "\n",
    "$$ \\frac{\\epsilon}{\\sigma} \\sim N_n(0, I_n)  $$\n",
    "\n",
    "\n",
    "$$ e^Te = (M\\epsilon)^TM\\epsilon = \\epsilon^T M \\epsilon $$\n",
    "\n",
    "$$ \\frac{e^Te}{\\sigma^2} = \\frac{\\epsilon}{\\sigma}^TM\\frac{\\epsilon}{\\sigma} \\sim \\chi^2(n-k) \\quad \\text{ given that $rank(M)=n-k$} $$\n",
    "\n",
    "Then, $\\frac{e^Te}{\\sigma^2}$ follows a $ \\chi^2(n-k)$ distribution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c32614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
